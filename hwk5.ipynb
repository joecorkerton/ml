{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform,randint, random, shuffle, sample\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0.0447362903978\n",
      "0.0239587140991\n"
     ]
    }
   ],
   "source": [
    "def descend(E_in, u, v, eta):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for the error surface\n",
    "    E(u, v) = (u*e^v - 2*v*e^-u)^2\n",
    "    eta = learning rate (= 0.1)\n",
    "    \"\"\"\n",
    "    tmp_u = u\n",
    "    tmp_v = v\n",
    "    \n",
    "    #grad(E_in) = dE/du + dE/dv\n",
    "    du = 2 * (tmp_u * np.exp(tmp_v) - 2*tmp_v*np.exp(-tmp_u)) * (np.exp(tmp_v) + 2*tmp_v*np.exp(-tmp_u))\n",
    "    dv = 2*(tmp_u*np.exp(tmp_v) - 2*tmp_v*np.exp(-tmp_u))*(tmp_u*np.exp(tmp_v) - 2*np.exp(-tmp_u))\n",
    "    #Update u and v\n",
    "    u = u - eta * du\n",
    "    v = v - eta * dv\n",
    "    E_in = (u *np.exp(v) - 2*v*np.exp(-u))**2\n",
    "    \n",
    "    return (E_in, u, v)\n",
    "\n",
    "#Initialise values\n",
    "u = 1\n",
    "v = 1\n",
    "E_in = (u *np.exp(v) - 2*v*np.exp(-u))**2\n",
    "n= 0\n",
    "\n",
    "while E_in > 1E-14:\n",
    "    n += 1\n",
    "    new_values = descend(E_in, u, v, 0.1)\n",
    "    u = new_values[1]\n",
    "    v = new_values[2]\n",
    "    E_in = new_values[0]\n",
    "    \n",
    "print n\n",
    "print u\n",
    "print v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-ordinate Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "6.29707589931\n",
      "-2.85230695408\n",
      "0.139813791996\n"
     ]
    }
   ],
   "source": [
    "def co_descend(E_in, u, v, eta):\n",
    "    \"\"\"\n",
    "    Perform coordinate descent for the error surface\n",
    "    E(u, v) = (u*e^v - 2*v*e^-u)^2\n",
    "    eta = learning rate (= 0.1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #grad(E_in) = dE/du + dE/dv\n",
    "    #First update u\n",
    "    du = 2 * (u * np.exp(v) - 2*v*np.exp(-u)) * (np.exp(v) + 2*v*np.exp(-u))\n",
    "    u = u - eta * du\n",
    "    #Use updated u to update v\n",
    "    dv = 2*(u*np.exp(v) - 2*v*np.exp(-u))*(u*np.exp(v) - 2*np.exp(-u))\n",
    "\n",
    "    v = v - eta * dv\n",
    "    E_in = (u *np.exp(v) - 2*v*np.exp(-u))**2\n",
    "    \n",
    "    return (E_in, u, v)\n",
    "\n",
    "#Initialise values\n",
    "u = 1\n",
    "v = 1\n",
    "E_in = (u *np.exp(v) - 2*v*np.exp(-u))**2\n",
    "n= 0\n",
    "\n",
    "while n < 15:\n",
    "    n += 1\n",
    "    new_values = co_descend(E_in, u, v, 0.1)\n",
    "    u = new_values[1]\n",
    "    v = new_values[2]\n",
    "    E_in = new_values[0]\n",
    "    \n",
    "print n\n",
    "print u\n",
    "print v\n",
    "print E_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "### Using stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340.09\n",
      "0.103527927403\n"
     ]
    }
   ],
   "source": [
    "#Generates a random line in the square [-1,1], and returns the gradient and intercept\n",
    "def f ():\n",
    "    #Generate two random points in the square\n",
    "    (x1,y1) = (uniform(-1,1),uniform(-1,1))\n",
    "    (x2,y2) = (uniform(-1,1),uniform(-1,1))\n",
    "    \n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    c = y1 - m * x1\n",
    "    \n",
    "    return (m, c)\n",
    "\n",
    "#Generate an array of n lists, containing a point in a square, uniformly distributed in [-1,1]\n",
    "def arrays(n):\n",
    "    if n == 0:\n",
    "        return np.array([])\n",
    "    ar = np.ones((n,3))\n",
    "    n -= 1\n",
    "    while n >= 0:\n",
    "        #x_0 = 1\n",
    "        ar[n] = [1, uniform(-1,1), uniform (-1,1)]\n",
    "        n-= 1\n",
    "    return ar\n",
    "\n",
    "#for a given x and f, finds the values of y\n",
    "def yarray(x, m, c, n):\n",
    "    ys = np.zeros(n)\n",
    "    for i in range(len(x)):\n",
    "        if x[i, 2] > m * x [i, 1] + c:\n",
    "            ys[i] = 1\n",
    "        else:\n",
    "            ys[i] = -1\n",
    "    return ys\n",
    "\n",
    "#Return the out of sample error E_out\n",
    "def e_out (w, x, y):\n",
    "    total = 0\n",
    "    for i in range(len(x)):\n",
    "        total += np.log(1 + np.exp(- y[i] * np.dot(x[i], w))) \n",
    "    \n",
    "    return total / len(x)\n",
    "\n",
    "#Apply logistic regression\n",
    "def log_reg (w, x, y):\n",
    "    for i in range(len(x)):\n",
    "        dw = - y[i] * x[i] / (1 + np.exp(y[i] * np.dot(x[i], w)))\n",
    "        #update w. eta = 0.01\n",
    "        w = w - 0.01 * dw\n",
    "    return w\n",
    "\n",
    "N = 0\n",
    "E_out = 0\n",
    "#Repeat experiment 100 times and take average values\n",
    "for j in range(100):\n",
    "\n",
    "    #Initialise values\n",
    "    (m, c) = f()\n",
    "    #N = 100\n",
    "    x = arrays(100)\n",
    "    y = yarray(x, m, c, 100)\n",
    "    #Initial weight vector\n",
    "    w = np.zeros(3)\n",
    "    n = 0\n",
    "    #Convergence when difference in weights between epochs is less than 0.01\n",
    "    convergence = False\n",
    "\n",
    "    while not convergence:\n",
    "        n += 1\n",
    "        #Permute the order of x and y values\n",
    "        x_shuf = np.ones((100, 3))\n",
    "        y_shuf = np.ones(100)\n",
    "        index = range(len(x))\n",
    "        shuffle(index)\n",
    "        k = 0\n",
    "        for i in index:\n",
    "            x_shuf[i] = x[k]\n",
    "            y_shuf = y[k]\n",
    "            k += 1\n",
    "        w_old = w\n",
    "        w = log_reg(w, x, y)\n",
    "        if np.linalg.norm(w - w_old) < 0.01:\n",
    "            convergence = True\n",
    "    \n",
    "    #Calculate E_out\n",
    "    x_out = arrays(1000)\n",
    "    y_out = yarray(x_out, m, c, 1000)\n",
    "    error = e_out(w, x_out, y_out)\n",
    "    N += n\n",
    "    E_out += error\n",
    "\n",
    "print N/100\n",
    "print E_out/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print np.dot([3, 4], [1, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
